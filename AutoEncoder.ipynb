{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Training AutoEncoder\n"
      ],
      "metadata": {
        "id": "K6iWBmwPb-DR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OyEUkK3Wc_R",
        "outputId": "ccdc0b56-b484-4079-a516-f343e51e6e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 793 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 5s/step - loss: 0.0217\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 0.0139\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 0.0042\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - loss: 0.0020\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.0015\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - loss: 0.0020\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 148ms/step - loss: 0.0016\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.0012\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.0011\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 9.6163e-04\n",
            "Autoencoder saved at: /content/drive/MyDrive/skin_cancer/autoencoder.keras\n",
            "Encoder saved at: /content/drive/MyDrive/skin_cancer/encoder.keras\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "autoencoder_model_path = '/content/drive/MyDrive/skin_cancer/autoencoder.keras'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/encoder.keras'\n",
        "\n",
        "# Autoencoder Architecture\n",
        "input_shape = (224, 224, 3)  # Image size\n",
        "\n",
        "# Encoder\n",
        "input_img = Input(shape=input_shape)\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "bottleneck = MaxPooling2D((2,2), padding='same', name=\"encoder_output\")(x)  # Bottleneck Layer\n",
        "\n",
        "# Decoder\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(bottleneck)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "output_img = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)  # Reconstructed Image\n",
        "\n",
        "# Build Autoencoder Model\n",
        "autoencoder = Model(input_img, output_img)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Extract Encoder Model\n",
        "encoder = Model(input_img, bottleneck)\n",
        "\n",
        "# Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='input',  # Change from 'None' to 'input' for autoencoder\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Train Autoencoder\n",
        "autoencoder.fit(\n",
        "    data_generator,\n",
        "    epochs=10,  # Adjust based on performance\n",
        "    steps_per_epoch=len(data_generator)\n",
        ")\n",
        "\n",
        "# Save Models\n",
        "autoencoder.save(autoencoder_model_path)\n",
        "encoder.save(encoder_model_path)\n",
        "print(f\"Autoencoder saved at: {autoencoder_model_path}\")\n",
        "print(f\"Encoder saved at: {encoder_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting features from auto encoder"
      ],
      "metadata": {
        "id": "mmzlaNbib8xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "encoder_path = '/content/drive/MyDrive/skin_cancer/encoder.keras'\n",
        "output_csv_path = '/content/drive/MyDrive/skin_cancer/autoencoder_features_with_labels.csv'\n",
        "\n",
        "# Load the trained encoder\n",
        "try:\n",
        "    encoder = load_model(encoder_path)\n",
        "    print(f\"Encoder model loaded successfully from {encoder_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading encoder: {e}\")\n",
        "    raise\n",
        "\n",
        "# Create Data Generator for the dataset\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0)  # Normalize pixel values\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(224, 224),  # Ensure it matches training dimensions\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Get labels\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Extract latent features\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "for batch_idx, (batch, labels) in enumerate(data_generator):\n",
        "    features = encoder.predict(batch, verbose=0)  # Get bottleneck features\n",
        "    all_features.append(features)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "    # Stop after processing all images\n",
        "    if (batch_idx + 1) * data_generator.batch_size >= data_generator.n:\n",
        "        break\n",
        "\n",
        "# Convert to structured format\n",
        "if all_features:\n",
        "    all_features = np.concatenate(all_features, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    reshaped_features = all_features.reshape(all_features.shape[0], -1)  # Flatten\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(reshaped_features)\n",
        "    df['label'] = all_labels.astype(int)  # Append labels\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Features with labels saved to {output_csv_path}\")\n",
        "else:\n",
        "    print(\"No features extracted!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGS_iU4qe2RG",
        "outputId": "82de568b-d96e-4cc2-f2c4-32c444290479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Encoder model loaded successfully from /content/drive/MyDrive/skin_cancer/encoder.keras\n",
            "Found 793 images belonging to 2 classes.\n",
            "Features with labels saved to /content/drive/MyDrive/skin_cancer/autoencoder_features_with_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for Running SVM, NN, and RF"
      ],
      "metadata": {
        "id": "Kp8VCKntgZYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Path to extracted features CSV\n",
        "csv_path = '/content/drive/MyDrive/skin_cancer/autoencoder_features_with_labels.csv'\n",
        "\n",
        "# Load features and labels\n",
        "df = pd.read_csv(csv_path)\n",
        "X = df.drop(columns=['label']).values  # Features\n",
        "y = df['label'].values  # Labels\n",
        "\n",
        "# Split dataset into train and test (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize classifiers\n",
        "svm_model = SVC(kernel='rbf', C=1.0, random_state=42)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
        "\n",
        "# Train models\n",
        "svm_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "nn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_nn = nn_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
        "\n",
        "# Print results\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"Neural Network Accuracy: {accuracy_nn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGwv_DaYgaqL",
        "outputId": "83c94f18-26da-4fd6-baad-d31a8347de53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.7296\n",
            "Random Forest Accuracy: 0.7107\n",
            "Neural Network Accuracy: 0.7358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving autoencoder"
      ],
      "metadata": {
        "id": "LbcPCaYbhQGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   20 epochs\n",
        "*    More Neurons in Latent Space (256): Helps retain more useful features.\n",
        "*   More layers added for better feature extraction.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5sPYrDschTrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()\n"
      ],
      "metadata": {
        "id": "0G0a5PSUlhcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "autoencoder_model_path = '/content/drive/MyDrive/skin_cancer/improved_autoencoder.keras'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved_encoder.keras'\n",
        "\n",
        "# Clear GPU memory before training\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Autoencoder Architecture (Memory Optimized)\n",
        "input_shape = (160, 160, 3)  # Reduce image size to save memory\n",
        "\n",
        "# Encoder\n",
        "input_img = Input(shape=input_shape)\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "bottleneck = Conv2D(256, (3,3), activation='relu', padding='same', name=\"encoder_output\")(x)\n",
        "\n",
        "# Decoder\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(bottleneck)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "output_img = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Build Autoencoder Model\n",
        "autoencoder = Model(input_img, output_img)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Extract Encoder Model\n",
        "encoder = Model(input_img, bottleneck)\n",
        "\n",
        "# Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),  # Reduced image size\n",
        "    batch_size=8,  # Reduced batch size\n",
        "    class_mode='input',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Train Autoencoder\n",
        "autoencoder.fit(\n",
        "    data_generator,\n",
        "    epochs=15,  # Reduce epochs to save resources\n",
        "    steps_per_epoch=200  # Limit steps per epoch to avoid OOM\n",
        ")\n",
        "\n",
        "# Save Models\n",
        "autoencoder.save(autoencoder_model_path)\n",
        "encoder.save(encoder_model_path)\n",
        "print(f\"Autoencoder saved at: {autoencoder_model_path}\")\n",
        "print(f\"Encoder saved at: {encoder_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdxYoLC6heXV",
        "outputId": "835c383d-4c72-4a51-9254-3ee13edfc5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 793 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - loss: 0.0174\n",
            "Epoch 2/15\n",
            "\u001b[1m  5/200\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.0083"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0057\n",
            "Epoch 3/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0045\n",
            "Epoch 4/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0045\n",
            "Epoch 5/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0046\n",
            "Epoch 6/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0040\n",
            "Epoch 7/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0037\n",
            "Epoch 8/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0037\n",
            "Epoch 9/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0039\n",
            "Epoch 10/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0033\n",
            "Epoch 11/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0036\n",
            "Epoch 12/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0032\n",
            "Epoch 13/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0033\n",
            "Epoch 14/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0033\n",
            "Epoch 15/15\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0030\n",
            "Autoencoder saved at: /content/drive/MyDrive/skin_cancer/improved_autoencoder.keras\n",
            "Encoder saved at: /content/drive/MyDrive/skin_cancer/improved_encoder.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved_encoder.keras'\n",
        "output_csv_path = '/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv'\n",
        "\n",
        "# Load the trained encoder model\n",
        "encoder = load_model(encoder_model_path)\n",
        "\n",
        "# Create ImageDataGenerator for loading images (without augmentation)\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),  # Ensure this matches the training image size\n",
        "    batch_size=1,  # Process one image at a time to avoid memory issues\n",
        "    class_mode=None,  # No labels needed as we extract features\n",
        "    shuffle=False  # Maintain order for correct label mapping\n",
        ")\n",
        "\n",
        "# Extract labels (folder names represent class labels)\n",
        "class_indices = data_generator.class_indices\n",
        "label_map = {v: k for k, v in class_indices.items()}  # Convert index to class name\n",
        "\n",
        "# Extract features\n",
        "features_list = []\n",
        "labels_list = []\n",
        "\n",
        "for i in range(len(data_generator)):\n",
        "    image = data_generator[i]  # Get image\n",
        "    feature_vector = encoder.predict(image, verbose=0).flatten()  # Extract features\n",
        "    label_index = data_generator.classes[i]  # Get corresponding label index\n",
        "    label_name = label_map[label_index]  # Convert index to class name\n",
        "\n",
        "    features_list.append(feature_vector)\n",
        "    labels_list.append(label_name)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "feature_columns = [f'feature_{i}' for i in range(len(features_list[0]))]\n",
        "df = pd.DataFrame(features_list, columns=feature_columns)\n",
        "df['label'] = labels_list  # Add label column\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "print(f\"Features saved successfully at: {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ot8DvYuhyUD",
        "outputId": "a5ddc553-54f3-44cd-b97a-8b598d1bc2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 793 images belonging to 2 classes.\n",
            "Features saved successfully at: /content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Path to extracted features CSV\n",
        "csv_path = '/content/drive/MyDrive/skin_cancer/l_improved_autoencoder_features_with_labels.csv'\n",
        "\n",
        "# Load features and labels\n",
        "df = pd.read_csv(csv_path)\n",
        "X = df.drop(columns=['label']).values  # Features\n",
        "y = df['label'].values  # Labels\n",
        "\n",
        "# Split dataset into train and test (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize classifiers\n",
        "svm_model = SVC(kernel='rbf', C=1.0, random_state=42)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
        "\n",
        "# Train models\n",
        "svm_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "nn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_nn = nn_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
        "\n",
        "# Print results\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"Neural Network Accuracy: {accuracy_nn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNRRdXGWiB_e",
        "outputId": "fdfe71e4-56d7-47f0-a9f6-f9da3864173e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.7421\n",
            "Random Forest Accuracy: 0.7925\n",
            "Neural Network Accuracy: 0.7799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved_encoder.keras'\n",
        "output_csv_path = '/content/drive/MyDrive/skin_cancer/l_improved_autoencoder_features_with_labels.csv'\n",
        "\n",
        "# Load the trained encoder model\n",
        "encoder = load_model(encoder_model_path)\n",
        "\n",
        "# Identify the correct bottleneck layer\n",
        "bottleneck_layer_name = 'encoder_output'  # Change this if needed\n",
        "bottleneck_model = Model(inputs=encoder.input, outputs=encoder.get_layer(bottleneck_layer_name).output)\n",
        "\n",
        "# Create ImageDataGenerator for loading images (without augmentation)\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),  # Ensure this matches the training image size\n",
        "    batch_size=1,  # Process one image at a time to avoid memory issues\n",
        "    class_mode=None,  # No labels needed as we extract features\n",
        "    shuffle=False  # Maintain order for correct label mapping\n",
        ")\n",
        "\n",
        "# Extract labels (folder names represent class labels)\n",
        "class_indices = data_generator.class_indices\n",
        "label_map = {v: k for k, v in class_indices.items()}  # Convert index to class name\n",
        "\n",
        "# Extract features\n",
        "features_list = []\n",
        "labels_list = []\n",
        "\n",
        "for i in range(len(data_generator)):\n",
        "    image = data_generator[i]  # Get image\n",
        "    feature_vector = bottleneck_model.predict(image, verbose=0).flatten()  # Extract bottleneck features\n",
        "    label_index = data_generator.classes[i]  # Get corresponding label index\n",
        "    label_name = label_map[label_index]  # Convert index to class name\n",
        "\n",
        "    features_list.append(feature_vector)\n",
        "    labels_list.append(label_name)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "feature_columns = [f'feature_{i}' for i in range(len(features_list[0]))]\n",
        "df = pd.DataFrame(features_list, columns=feature_columns)\n",
        "df['label'] = labels_list  # Add label column\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "print(f\"Latent features saved successfully at: {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCri5MG5B99o",
        "outputId": "5b53fab5-00f9-401c-a0e9-5a73baed7b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 793 images belonging to 2 classes.\n",
            "Latent features saved successfully at: /content/drive/MyDrive/skin_cancer/l_improved_autoencoder_features_with_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concatenate resnet and autoencoder"
      ],
      "metadata": {
        "id": "ZkljWoHR5A6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### no pca"
      ],
      "metadata": {
        "id": "cKEYNGbKFVea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load extracted features from CSVs\n",
        "autoencoder_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv')\n",
        "resnet_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/resnet50_conv5_block2_out_features_with_labels.csv')\n",
        "\n",
        "# Convert autoencoder labels to match ResNet format (1.0 for benign, 0.0 for malignant)\n",
        "label_mapping = {'benign': 1.0, 'malignant': 0.0}\n",
        "autoencoder_features['label'] = autoencoder_features['label'].map(label_mapping)\n",
        "\n",
        "# Ensure labels are consistent across both datasets\n",
        "y = autoencoder_features['label'].values  # Both feature sets now have 1.0 for benign, 0.0 for malignant\n",
        "\n",
        "# Drop label columns to extract feature matrices\n",
        "X_auto = autoencoder_features.drop(columns=['label']).values\n",
        "X_resnet = resnet_features.drop(columns=['label']).values\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_auto_scaled = scaler.fit_transform(X_auto)\n",
        "X_resnet_scaled = scaler.fit_transform(X_resnet)\n",
        "\n",
        "# Concatenate the two feature sets\n",
        "X_combined = np.concatenate((X_auto_scaled, X_resnet_scaled), axis=1)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "svm = SVC(kernel='rbf', C=1.0)\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "nn = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', max_iter=1000)\n",
        "\n",
        "# Train models\n",
        "svm.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_nn = nn.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVp6GFvN5HA3",
        "outputId": "85350eeb-977d-4db6-ada6-b22b5f3a3011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA"
      ],
      "metadata": {
        "id": "zzsKwSzcFYM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load extracted features from CSVs\n",
        "autoencoder_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv')\n",
        "resnet_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/resnet50_conv5_block2_out_features_with_labels.csv')\n",
        "\n",
        "# Convert autoencoder labels to match ResNet format (1.0 for benign, 0.0 for malignant)\n",
        "label_mapping = {'benign': 1.0, 'malignant': 0.0}\n",
        "autoencoder_features['label'] = autoencoder_features['label'].map(label_mapping)\n",
        "\n",
        "# Ensure labels are consistent across both datasets\n",
        "y = autoencoder_features['label'].values  # Now both feature sets have 1.0 for benign, 0.0 for malignant\n",
        "\n",
        "# Drop label columns to extract feature matrices\n",
        "X_auto = autoencoder_features.drop(columns=['label']).values\n",
        "X_resnet = resnet_features.drop(columns=['label']).values\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_auto_scaled = scaler.fit_transform(X_auto)\n",
        "X_resnet_scaled = scaler.fit_transform(X_resnet)\n",
        "\n",
        "# Apply PCA to reduce feature dimensions\n",
        "pca_auto = PCA(n_components=100)  # Adjust based on variance explained\n",
        "pca_resnet = PCA(n_components=100)\n",
        "\n",
        "X_auto_pca = pca_auto.fit_transform(X_auto_scaled)\n",
        "X_resnet_pca = pca_resnet.fit_transform(X_resnet_scaled)\n",
        "\n",
        "# Concatenate the reduced feature sets\n",
        "X_combined = np.concatenate((X_auto_pca, X_resnet_pca), axis=1)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "svm = SVC(kernel='rbf', C=1.0)\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "nn = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', max_iter=1000)\n",
        "\n",
        "# Train models\n",
        "svm.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_nn = nn.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNbCg-_X-gtS",
        "outputId": "32bad3e6-e005-4243-d19d-d6b863ba49c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "SVM Accuracy: 0.7735849056603774\n",
            "Random Forest Accuracy: 0.7735849056603774\n",
            "Neural Network Accuracy: 0.7610062893081762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "87sG1pqYFgST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load extracted features from CSVs\n",
        "autoencoder_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv')\n",
        "resnet_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/resnet50_conv5_block2_out_features_with_labels.csv')\n",
        "\n",
        "# Convert autoencoder labels to match ResNet format (1.0 for benign, 0.0 for malignant)\n",
        "label_mapping = {'benign': 1.0, 'malignant': 0.0}\n",
        "autoencoder_features['label'] = autoencoder_features['label'].map(label_mapping)\n",
        "\n",
        "# Ensure labels are consistent across both datasets\n",
        "y = autoencoder_features['label'].values  # Now both feature sets have 1.0 for benign, 0.0 for malignant\n",
        "\n",
        "# Drop label columns to extract feature matrices\n",
        "X_auto = autoencoder_features.drop(columns=['label']).values\n",
        "X_resnet = resnet_features.drop(columns=['label']).values\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_auto_scaled = scaler.fit_transform(X_auto)\n",
        "X_resnet_scaled = scaler.fit_transform(X_resnet)\n",
        "\n",
        "# Concatenate both feature sets\n",
        "X_combined = np.concatenate((X_auto_scaled, X_resnet_scaled), axis=1)\n",
        "\n",
        "# Apply LDA to reduce dimensions (LDA components <= number of classes - 1, so here 1 component)\n",
        "lda = LDA(n_components=1)\n",
        "X_lda = lda.fit_transform(X_combined, y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "svm = SVC(kernel='rbf', C=1.0)\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "nn = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', max_iter=1000)\n",
        "\n",
        "# Train models\n",
        "svm.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_nn = nn.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLiW_zm0E73E",
        "outputId": "b8cce5db-5e87-47d1-9c67-e1ee7cbcab31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### umap"
      ],
      "metadata": {
        "id": "6VPxpDLdOWzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch  # GPU Acceleration\n",
        "import gc  # Garbage Collection to Free RAM\n",
        "from cuml import UMAP  # GPU-accelerated UMAP\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "# ğŸš€ Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ğŸ”— Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ğŸ“‚ Load extracted features\n",
        "autoencoder_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv')\n",
        "resnet_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/resnet50_conv5_block2_out_features_with_labels.csv')\n",
        "\n",
        "# ğŸ”„ Convert labels\n",
        "label_mapping = {'benign': 1.0, 'malignant': 0.0}\n",
        "autoencoder_features['label'] = autoencoder_features['label'].map(label_mapping)\n",
        "y = autoencoder_features['label'].values  # Extract labels\n",
        "\n",
        "# ğŸ“Š Extract feature matrices\n",
        "X_auto = autoencoder_features.drop(columns=['label']).values\n",
        "X_resnet = resnet_features.drop(columns=['label']).values\n",
        "\n",
        "# ğŸ“ Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_auto_scaled = scaler.fit_transform(X_auto)\n",
        "X_resnet_scaled = scaler.fit_transform(X_resnet)\n",
        "\n",
        "# ğŸ”— Combine features\n",
        "X_combined = np.concatenate((X_auto_scaled, X_resnet_scaled), axis=1)\n",
        "\n",
        "# ğŸ§  **Apply GPU-accelerated UMAP for Feature Reduction**\n",
        "umap = UMAP(n_components=50, random_state=42)  # Reduce to 50 features\n",
        "X_umap = umap.fit_transform(X_combined)\n",
        "\n",
        "# ğŸš® Free RAM\n",
        "del X_auto, X_resnet, X_auto_scaled, X_resnet_scaled, X_combined\n",
        "gc.collect()\n",
        "\n",
        "# ğŸ›‘ **Fixing Data Leakage**\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_umap, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
        ")\n",
        "\n",
        "# ğŸš¨ Data Check\n",
        "print(f\"Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
        "\n",
        "# ğŸ”¥ Train Classifiers\n",
        "svm = SVC(kernel='rbf', C=1.0)\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# ğŸ¯ Predictions\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# ğŸ“Š Accuracy Results\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "\n",
        "# ğŸ§  **GPU-based Neural Network using PyTorch**\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ğŸ”¥ Define Neural Network (GPU Accelerated)\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(50, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# ğŸš€ Move Model to GPU\n",
        "model = SimpleNN().to(device)\n",
        "\n",
        "# ğŸ¯ Loss & Optimizer\n",
        "criterion = nn.BCELoss()  # Binary Classification Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ğŸ‹ï¸ Train Model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# ğŸ“Š Evaluate Model\n",
        "model.eval()\n",
        "y_pred_nn = model(X_test_tensor).cpu().detach().numpy().round()\n",
        "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nFxlVhOHrvI",
        "outputId": "9240c51c-f522-48d6-db02-9557ea6f39e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train Shape: (634, 50), Test Shape: (159, 50)\n",
            "SVM Accuracy: 0.6981132075471698\n",
            "Random Forest Accuracy: 0.7421383647798742\n",
            "Epoch 1/10, Loss: 0.7019618153572083\n",
            "Epoch 2/10, Loss: 0.6786752939224243\n",
            "Epoch 3/10, Loss: 0.7452600598335266\n",
            "Epoch 4/10, Loss: 0.6410877108573914\n",
            "Epoch 5/10, Loss: 0.6243048906326294\n",
            "Epoch 6/10, Loss: 0.665518581867218\n",
            "Epoch 7/10, Loss: 0.4798668622970581\n",
            "Epoch 8/10, Loss: 0.5306162238121033\n",
            "Epoch 9/10, Loss: 0.5745664238929749\n",
            "Epoch 10/10, Loss: 0.4457313120365143\n",
            "Neural Network Accuracy: 0.7232704402515723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB"
      ],
      "metadata": {
        "id": "bxJm0fwsQNLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch  # For GPU checks\n",
        "import gc  # Garbage Collection to Free RAM\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier  # GPU-accelerated XGBoost\n",
        "from google.colab import drive\n",
        "\n",
        "# ğŸš€ Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ğŸ”— Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ğŸ“‚ Load extracted features\n",
        "autoencoder_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv')\n",
        "resnet_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/resnet50_conv5_block2_out_features_with_labels.csv')\n",
        "\n",
        "# ğŸ”„ Convert labels\n",
        "label_mapping = {'benign': 1.0, 'malignant': 0.0}\n",
        "autoencoder_features['label'] = autoencoder_features['label'].map(label_mapping)\n",
        "y = autoencoder_features['label'].values  # Extract labels\n",
        "\n",
        "# ğŸ“Š Extract feature matrices\n",
        "X_auto = autoencoder_features.drop(columns=['label']).values\n",
        "X_resnet = resnet_features.drop(columns=['label']).values\n",
        "\n",
        "# ğŸ“ Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_auto_scaled = scaler.fit_transform(X_auto)\n",
        "X_resnet_scaled = scaler.fit_transform(X_resnet)\n",
        "\n",
        "# ğŸ”— Combine features\n",
        "X_combined = np.concatenate((X_auto_scaled, X_resnet_scaled), axis=1)\n",
        "\n",
        "# ğŸ† **Feature Selection Using Mutual Information**\n",
        "num_features = 100  # Keep only the top 100 most important features\n",
        "mi_scores = mutual_info_classif(X_combined, y)\n",
        "top_features_idx = np.argsort(mi_scores)[-num_features:]  # Select top N features\n",
        "X_selected = X_combined[:, top_features_idx]\n",
        "\n",
        "# ğŸš® Free RAM\n",
        "del X_auto, X_resnet, X_auto_scaled, X_resnet_scaled, X_combined\n",
        "gc.collect()\n",
        "\n",
        "# ğŸ›‘ **Fixing Data Leakage**\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
        ")\n",
        "\n",
        "# ğŸš¨ Data Check\n",
        "print(f\"Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
        "\n",
        "# ğŸ”¥ **Train GPU-Accelerated XGBoost Classifier**\n",
        "xgb = XGBClassifier(\n",
        "    tree_method='gpu_hist',  # Enable GPU acceleration\n",
        "    predictor='gpu_predictor',\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5\n",
        ")\n",
        "\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# ğŸ¯ Predictions\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# ğŸ“Š Accuracy Results\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pyfq-sqQLPe",
        "outputId": "e70eb534-1fdb-49ea-8434-b16288468003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if X_train and X_test have overlapping values\n",
        "print(\"Overlap in train & test:\", np.intersect1d(X_train.cpu().numpy(), X_test.cpu().numpy()).size > 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUd4KbpnJ8px",
        "outputId": "f16f7895-0a82-4e2f-967f-e6928ac7a74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap in train & test: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the feature files\n",
        "autoencoder_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/improved_autoencoder_features_with_labels.csv')\n",
        "resnet_features = pd.read_csv('/content/drive/MyDrive/skin_cancer/resnet50_conv5_block2_out_features_with_labels.csv')\n",
        "\n",
        "# Print first few rows of the labels column\n",
        "print(\"Autoencoder Labels:\")\n",
        "print(autoencoder_features['label'].head())  # Assuming 'label' is the column name\n",
        "\n",
        "print(\"\\nResNet50 Labels:\")\n",
        "print(resnet_features['label'].head())  # Assuming 'label' is the column name\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMTm_NQy7s-7",
        "outputId": "29a3b8b0-7036-445e-e970-bc1a57a96dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder Labels:\n",
            "0    benign\n",
            "1    benign\n",
            "2    benign\n",
            "3    benign\n",
            "4    benign\n",
            "Name: label, dtype: object\n",
            "\n",
            "ResNet50 Labels:\n",
            "0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: label, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improve 2.0"
      ],
      "metadata": {
        "id": "G-BNtyGvqCJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Paths\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved_encoder.keras'\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "output_csv_path = '/content/drive/MyDrive/skin_cancer/advanced_features.csv'\n",
        "\n",
        "# Load the trained encoder model\n",
        "encoder = load_model(encoder_model_path)\n",
        "\n",
        "# Extract the last layer (bottleneck)\n",
        "bottleneck_layer = encoder.get_layer(\"encoder_output\").output\n",
        "feature_extractor = Model(inputs=encoder.input, outputs=bottleneck_layer)\n",
        "\n",
        "# Image Generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder, target_size=(160, 160), batch_size=1, class_mode=None, shuffle=False\n",
        ")\n",
        "\n",
        "# Map class indices to labels\n",
        "label_map = {v: k for k, v in data_generator.class_indices.items()}\n",
        "\n",
        "# Extract features\n",
        "features_list = []\n",
        "labels_list = []\n",
        "\n",
        "for i in range(len(data_generator)):\n",
        "    image = data_generator[i]\n",
        "    feature_vector = feature_extractor.predict(image, verbose=0).flatten()  # Extract bottleneck features\n",
        "\n",
        "    label_name = label_map[data_generator.classes[i]]\n",
        "\n",
        "    features_list.append(feature_vector)\n",
        "    labels_list.append(label_name)\n",
        "\n",
        "# Convert to DataFrame\n",
        "feature_columns = [f'feature_{i}' for i in range(len(features_list[0]))]\n",
        "df = pd.DataFrame(features_list, columns=feature_columns)\n",
        "df['label'] = labels_list\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=100)  # Reduce to 100 principal features\n",
        "pca_features = pca.fit_transform(df.iloc[:, :-1])\n",
        "df_pca = pd.DataFrame(pca_features, columns=[f'pca_{i}' for i in range(100)])\n",
        "df_pca['label'] = labels_list\n",
        "\n",
        "# Save to CSV\n",
        "df_pca.to_csv(output_csv_path, index=False)\n",
        "print(f\"Optimized features saved at: {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnEtsW7WqEQa",
        "outputId": "e8081002-12a6-4bec-f68e-5a565f887a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 793 images belonging to 2 classes.\n",
            "Optimized features saved at: /content/drive/MyDrive/skin_cancer/advanced_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the extracted features CSV\n",
        "csv_path = '/content/drive/MyDrive/skin_cancer/advanced_features.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.iloc[:, :-1].values  # All feature columns\n",
        "y = df['label'].values  # Labels\n",
        "\n",
        "# Encode labels (convert categorical to numeric)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split dataset into training & testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- Train SVM Classifier ---\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma='scale')  # Tuned hyperparameters\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "\n",
        "# --- Train Random Forest Classifier ---\n",
        "rf_model = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# --- Train Neural Network (MLP) ---\n",
        "nn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(len(np.unique(y_encoded)), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train, epochs=30, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Neural Network Accuracy: {nn_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xnyFKzLrDhC",
        "outputId": "5540651f-bc43-4def-af2d-ffdd52961c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.7296\n",
            "Random Forest Accuracy: 0.7484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 65ms/step - accuracy: 0.5456 - loss: 0.7852 - val_accuracy: 0.6164 - val_loss: 0.6479\n",
            "Epoch 2/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6464 - loss: 0.6197 - val_accuracy: 0.6918 - val_loss: 0.6219\n",
            "Epoch 3/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7474 - loss: 0.5395 - val_accuracy: 0.7170 - val_loss: 0.5759\n",
            "Epoch 4/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7832 - loss: 0.4582 - val_accuracy: 0.7358 - val_loss: 0.5880\n",
            "Epoch 5/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8528 - loss: 0.3833 - val_accuracy: 0.7233 - val_loss: 0.5816\n",
            "Epoch 6/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8635 - loss: 0.3447 - val_accuracy: 0.7296 - val_loss: 0.6077\n",
            "Epoch 7/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8819 - loss: 0.3103 - val_accuracy: 0.7296 - val_loss: 0.6231\n",
            "Epoch 8/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9102 - loss: 0.2697 - val_accuracy: 0.7421 - val_loss: 0.6236\n",
            "Epoch 9/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9038 - loss: 0.2673 - val_accuracy: 0.7233 - val_loss: 0.6854\n",
            "Epoch 10/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9320 - loss: 0.1894 - val_accuracy: 0.7358 - val_loss: 0.7366\n",
            "Epoch 11/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9162 - loss: 0.2169 - val_accuracy: 0.7547 - val_loss: 0.7642\n",
            "Epoch 12/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9145 - loss: 0.2227 - val_accuracy: 0.7484 - val_loss: 0.7607\n",
            "Epoch 13/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9346 - loss: 0.1736 - val_accuracy: 0.7358 - val_loss: 0.8684\n",
            "Epoch 14/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9499 - loss: 0.1483 - val_accuracy: 0.7484 - val_loss: 0.9200\n",
            "Epoch 15/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9530 - loss: 0.1225 - val_accuracy: 0.7673 - val_loss: 0.9796\n",
            "Epoch 16/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9438 - loss: 0.1353 - val_accuracy: 0.7547 - val_loss: 1.0175\n",
            "Epoch 17/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9578 - loss: 0.1237 - val_accuracy: 0.7484 - val_loss: 1.1481\n",
            "Epoch 18/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9721 - loss: 0.1011 - val_accuracy: 0.7233 - val_loss: 1.0812\n",
            "Epoch 19/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9558 - loss: 0.1226 - val_accuracy: 0.7484 - val_loss: 1.0498\n",
            "Epoch 20/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9601 - loss: 0.0974 - val_accuracy: 0.7547 - val_loss: 1.1218\n",
            "Epoch 21/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9793 - loss: 0.0726 - val_accuracy: 0.7421 - val_loss: 1.1727\n",
            "Epoch 22/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9605 - loss: 0.0967 - val_accuracy: 0.7421 - val_loss: 1.1995\n",
            "Epoch 23/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9671 - loss: 0.0779 - val_accuracy: 0.7358 - val_loss: 1.2354\n",
            "Epoch 24/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9732 - loss: 0.0770 - val_accuracy: 0.7421 - val_loss: 1.1712\n",
            "Epoch 25/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9857 - loss: 0.0530 - val_accuracy: 0.7421 - val_loss: 1.2112\n",
            "Epoch 26/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0628 - val_accuracy: 0.7673 - val_loss: 1.1793\n",
            "Epoch 27/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9570 - loss: 0.0888 - val_accuracy: 0.7547 - val_loss: 1.1284\n",
            "Epoch 28/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9804 - loss: 0.0631 - val_accuracy: 0.7484 - val_loss: 1.1623\n",
            "Epoch 29/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9902 - loss: 0.0324 - val_accuracy: 0.7547 - val_loss: 1.2062\n",
            "Epoch 30/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9840 - loss: 0.0427 - val_accuracy: 0.7421 - val_loss: 1.2873\n",
            "Neural Network Accuracy: 0.7421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improve 3.0"
      ],
      "metadata": {
        "id": "b3AIvOLywAhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, UpSampling2D,\n",
        "                                     BatchNormalization, Dropout, GaussianNoise, Flatten, Dense, Reshape)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# ğŸš€ Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ğŸ—‚ Define dataset paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "autoencoder_model_path = '/content/drive/MyDrive/skin_cancer/improved2_autoencoder.keras'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved2_encoder.keras'\n",
        "\n",
        "# ğŸš€ Clear GPU memory before training\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# ğŸ¯ Define input image shape\n",
        "input_shape = (160, 160, 3)\n",
        "\n",
        "# ğŸ­ Encoder\n",
        "input_img = Input(shape=input_shape)\n",
        "x = GaussianNoise(0.1)(input_img)  # Adding noise for denoising AE\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)  # 160 â†’ 80\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)  # 80 â†’ 40\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)  # 40 â†’ 20\n",
        "\n",
        "x = Conv2D(256, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)  # 20 â†’ 10\n",
        "\n",
        "x = Flatten()(x)\n",
        "bottleneck = Dense(512, activation='relu', name=\"encoder_output\")(x)  # Only 512 neurons here\n",
        "\n",
        "# ğŸ­ Full Decoder (Fixed)\n",
        "x = Dense(10 * 10 * 256, activation='relu')(bottleneck)  # Expanding back\n",
        "x = Reshape((10, 10, 256))(x)\n",
        "\n",
        "x = Conv2D(256, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)  # 10 â†’ 20\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)  # 20 â†’ 40\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)  # 40 â†’ 80\n",
        "\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)  # 80 â†’ 160\n",
        "\n",
        "output_img = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)  # Final Output\n",
        "\n",
        "# âœ… Build Autoencoder Model\n",
        "autoencoder = Model(input_img, output_img)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# âœ… Extract Encoder Model\n",
        "encoder = Model(input_img, bottleneck)\n",
        "\n",
        "# ğŸ”„ Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),\n",
        "    batch_size=8,\n",
        "    class_mode='input',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "autoencoder.fit(\n",
        "    data_generator,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(data_generator) // 8\n",
        ")\n",
        "\n",
        "# ğŸ’¾ Save Models\n",
        "autoencoder.save(autoencoder_model_path)\n",
        "encoder.save(encoder_model_path)\n",
        "print(f\"Autoencoder saved at: {autoencoder_model_path}\")\n",
        "print(f\"Encoder saved at: {encoder_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YkQRqpqwFqo",
        "outputId": "9fe3f708-0f6f-4b21-bc07-78f10eb1dab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 793 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.0561\n",
            "Epoch 2/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0220\n",
            "Epoch 3/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 709ms/step - loss: 0.0168\n",
            "Epoch 4/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0173\n",
            "Epoch 5/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0143\n",
            "Epoch 6/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0144\n",
            "Epoch 7/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0128\n",
            "Epoch 8/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0098\n",
            "Epoch 9/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0075\n",
            "Epoch 10/20\n",
            "\u001b[1m 2/12\u001b[0m \u001b[32mâ”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0133"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0127\n",
            "Epoch 11/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0109\n",
            "Epoch 12/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0083\n",
            "Epoch 13/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0095\n",
            "Epoch 14/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0100\n",
            "Epoch 15/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0090\n",
            "Epoch 16/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0069\n",
            "Epoch 17/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0105\n",
            "Epoch 18/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0073\n",
            "Epoch 19/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0100\n",
            "Epoch 20/20\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0073\n",
            "Autoencoder saved at: /content/drive/MyDrive/skin_cancer/improved2_autoencoder.keras\n",
            "Encoder saved at: /content/drive/MyDrive/skin_cancer/improved2_encoder.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define paths\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved2_encoder.keras'\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "csv_output_path = '/content/drive/MyDrive/skin_cancer/improved2_extracted_features.csv'\n",
        "\n",
        "# Load trained encoder\n",
        "encoder = tf.keras.models.load_model(encoder_model_path)\n",
        "\n",
        "# Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),\n",
        "    batch_size=1,  # Process one image at a time\n",
        "    class_mode='binary',  # Ensure labels are available\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Extract labels correctly\n",
        "labels = data_generator.classes  # Get numeric labels\n",
        "class_indices = data_generator.class_indices  # Mapping class names to indices\n",
        "index_to_class = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
        "labels = [index_to_class[label] for label in labels]  # Convert to class names\n",
        "\n",
        "# Extract features\n",
        "features = encoder.predict(data_generator)\n",
        "\n",
        "# Convert features to DataFrame\n",
        "df_features = pd.DataFrame(features)\n",
        "df_features['label'] = labels  # Add labels column\n",
        "\n",
        "# Save to CSV\n",
        "df_features.to_csv(csv_output_path, index=False)\n",
        "print(f\"Features saved at: {csv_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W3CjDKu1DHe",
        "outputId": "1bc004b4-46d6-45fb-b971-140cf1c73504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 793 images belonging to 2 classes.\n",
            "\u001b[1m793/793\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step\n",
            "Features saved at: /content/drive/MyDrive/skin_cancer/improved2_extracted_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load extracted features\n",
        "csv_path = '/content/drive/MyDrive/skin_cancer/improved2_extracted_features.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=['label']).values  # Features\n",
        "y = df['label'].values  # Labels\n",
        "\n",
        "# Encode labels (convert class names to numbers)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Converts 'Benign' -> 0, 'Malignant' -> 1\n",
        "\n",
        "# Split dataset into training and testing (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train & Evaluate SVM\n",
        "svm_model = SVC(kernel='rbf', C=1.0)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "print(f\"ğŸ”¹ SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "\n",
        "# Train & Evaluate Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "print(f\"ğŸ”¹ Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# Train Neural Network (Simple Feedforward)\n",
        "nn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the neural network\n",
        "nn_model.fit(X_train, y_train, epochs=20, batch_size=8, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate Neural Network\n",
        "_, nn_accuracy = nn_model.evaluate(X_test, y_test)\n",
        "print(f\"ğŸ”¹ Neural Network Accuracy: {nn_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz9AVLYK4s7O",
        "outputId": "ed8ffdb8-5c52-42cd-d9e3-8fc73bbe9e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ SVM Accuracy: 0.5912\n",
            "ğŸ”¹ Random Forest Accuracy: 0.6792\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.5311 - loss: 6.8927 - val_accuracy: 0.5660 - val_loss: 1.3649\n",
            "Epoch 2/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5775 - loss: 2.9498 - val_accuracy: 0.5849 - val_loss: 1.1321\n",
            "Epoch 3/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5766 - loss: 0.9359 - val_accuracy: 0.5157 - val_loss: 1.2940\n",
            "Epoch 4/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6368 - loss: 0.9896 - val_accuracy: 0.5975 - val_loss: 0.7004\n",
            "Epoch 5/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6224 - loss: 1.0504 - val_accuracy: 0.5786 - val_loss: 1.0449\n",
            "Epoch 6/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6416 - loss: 0.7604 - val_accuracy: 0.6289 - val_loss: 0.6915\n",
            "Epoch 7/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6487 - loss: 0.6631 - val_accuracy: 0.6289 - val_loss: 0.6835\n",
            "Epoch 8/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6116 - loss: 0.6638 - val_accuracy: 0.6038 - val_loss: 1.0796\n",
            "Epoch 9/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6326 - loss: 0.7508 - val_accuracy: 0.5157 - val_loss: 1.0048\n",
            "Epoch 10/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5893 - loss: 0.7930 - val_accuracy: 0.5346 - val_loss: 0.7825\n",
            "Epoch 11/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6469 - loss: 0.6415 - val_accuracy: 0.5597 - val_loss: 0.7829\n",
            "Epoch 12/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6489 - loss: 0.6832 - val_accuracy: 0.5849 - val_loss: 0.6784\n",
            "Epoch 13/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6775 - loss: 0.6149 - val_accuracy: 0.6164 - val_loss: 0.6738\n",
            "Epoch 14/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6809 - loss: 0.6071 - val_accuracy: 0.5157 - val_loss: 0.8654\n",
            "Epoch 15/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6516 - loss: 0.6480 - val_accuracy: 0.6415 - val_loss: 0.6997\n",
            "Epoch 16/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6046 - loss: 0.6226 - val_accuracy: 0.5849 - val_loss: 0.7010\n",
            "Epoch 17/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6528 - loss: 0.6128 - val_accuracy: 0.5849 - val_loss: 0.7181\n",
            "Epoch 18/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6195 - loss: 0.6521 - val_accuracy: 0.5283 - val_loss: 1.1045\n",
            "Epoch 19/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6154 - loss: 0.6806 - val_accuracy: 0.5346 - val_loss: 0.7077\n",
            "Epoch 20/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6139 - loss: 0.6543 - val_accuracy: 0.5849 - val_loss: 0.6835\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.5747 - loss: 0.6931\n",
            "ğŸ”¹ Neural Network Accuracy: 0.5849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPROVE 4.0"
      ],
      "metadata": {
        "id": "G9-ZDvng5brd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, UpSampling2D,\n",
        "                                     BatchNormalization, Dropout, GaussianNoise, Flatten, Dense)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "autoencoder_model_path = '/content/drive/MyDrive/skin_cancer/improved3_autoencoder.keras'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved3_encoder.keras'\n",
        "\n",
        "# Clear GPU memory before training\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Autoencoder Architecture\n",
        "input_shape = (160, 160, 3)\n",
        "\n",
        "# Encoder\n",
        "input_img = Input(shape=input_shape)\n",
        "x = GaussianNoise(0.1)(input_img)  # Adding noise for denoising AE\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(256, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)  # Dropout for regularization\n",
        "bottleneck = Dense(256, activation='relu', name=\"encoder_output\")(x)\n",
        "\n",
        "# Decoder\n",
        "x = Dense(512, activation='relu')(bottleneck)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(160*160*3, activation='sigmoid')(x)\n",
        "x = tf.keras.layers.Reshape((160, 160, 3))(x)\n",
        "\n",
        "# Build Autoencoder Model\n",
        "autoencoder = Model(input_img, x)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Extract Encoder Model\n",
        "encoder = Model(input_img, bottleneck)\n",
        "\n",
        "# Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),\n",
        "    batch_size=8,\n",
        "    class_mode='input',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Train Autoencoder\n",
        "autoencoder.fit(\n",
        "    data_generator,\n",
        "    epochs=20,  # Slightly increased for better feature learning\n",
        "    steps_per_epoch=200\n",
        ")\n",
        "\n",
        "# Save Models\n",
        "autoencoder.save(autoencoder_model_path)\n",
        "encoder.save(encoder_model_path)\n",
        "print(f\"Autoencoder saved at: {autoencoder_model_path}\")\n",
        "print(f\"Encoder saved at: {encoder_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9vvqkbe5d7p",
        "outputId": "8a746a85-86ae-41dd-cb21-32b0774de4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 793 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 41ms/step - loss: 0.0151\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0099\n",
            "Epoch 3/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0094\n",
            "Epoch 4/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0093\n",
            "Epoch 5/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0099\n",
            "Epoch 6/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0097\n",
            "Epoch 7/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.0078\n",
            "Epoch 8/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0078\n",
            "Epoch 9/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0073\n",
            "Epoch 10/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 0.0081\n",
            "Epoch 11/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0078\n",
            "Epoch 12/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0082\n",
            "Epoch 13/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.0073\n",
            "Epoch 14/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0075\n",
            "Epoch 15/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0067\n",
            "Epoch 16/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0076\n",
            "Epoch 17/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0066\n",
            "Epoch 18/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0070\n",
            "Epoch 19/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0066\n",
            "Epoch 20/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0067\n",
            "Autoencoder saved at: /content/drive/MyDrive/skin_cancer/improved3_autoencoder.keras\n",
            "Encoder saved at: /content/drive/MyDrive/skin_cancer/improved3_encoder.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define paths\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/improved3_encoder.keras'\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "csv_output_path = '/content/drive/MyDrive/skin_cancer/improved3_extracted_features.csv'\n",
        "\n",
        "# Load trained encoder\n",
        "encoder = tf.keras.models.load_model(encoder_model_path)\n",
        "\n",
        "# Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),\n",
        "    batch_size=1,  # Process one image at a time\n",
        "    class_mode='binary',  # Ensure labels are available\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Extract labels correctly\n",
        "labels = data_generator.classes  # Get numeric labels\n",
        "class_indices = data_generator.class_indices  # Mapping class names to indices\n",
        "index_to_class = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
        "labels = [index_to_class[label] for label in labels]  # Convert to class names\n",
        "\n",
        "# Extract features\n",
        "features = encoder.predict(data_generator)\n",
        "\n",
        "# Convert features to DataFrame\n",
        "df_features = pd.DataFrame(features)\n",
        "df_features['label'] = labels  # Add labels column\n",
        "\n",
        "# Save to CSV\n",
        "df_features.to_csv(csv_output_path, index=False)\n",
        "print(f\"Features saved at: {csv_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kIjWUZ16Y9Q",
        "outputId": "9d49d22a-1581-4cb4-e9b0-8635656daa01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 793 images belonging to 2 classes.\n",
            "\u001b[1m793/793\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step\n",
            "Features saved at: /content/drive/MyDrive/skin_cancer/improved3_extracted_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load extracted features\n",
        "csv_path = '/content/drive/MyDrive/skin_cancer/improved3_extracted_features.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=['label']).values  # Features\n",
        "y = df['label'].values  # Labels\n",
        "\n",
        "# Encode labels (convert class names to numbers)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Converts 'Benign' -> 0, 'Malignant' -> 1\n",
        "\n",
        "# Split dataset into training and testing (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train & Evaluate SVM\n",
        "svm_model = SVC(kernel='rbf', C=1.0)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "print(f\"ğŸ”¹ SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "\n",
        "# Train & Evaluate Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "print(f\"ğŸ”¹ Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# Train Neural Network (Simple Feedforward)\n",
        "nn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the neural network\n",
        "nn_model.fit(X_train, y_train, epochs=20, batch_size=8, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate Neural Network\n",
        "_, nn_accuracy = nn_model.evaluate(X_test, y_test)\n",
        "print(f\"ğŸ”¹ Neural Network Accuracy: {nn_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHDbji1G6gJT",
        "outputId": "c6ff78c3-98bd-4578-c23e-7f72bc6af840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ SVM Accuracy: 0.5472\n",
            "ğŸ”¹ Random Forest Accuracy: 0.5031\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.5453 - loss: 0.8748 - val_accuracy: 0.6164 - val_loss: 0.7945\n",
            "Epoch 2/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5157 - loss: 0.7780 - val_accuracy: 0.5849 - val_loss: 0.8902\n",
            "Epoch 3/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5836 - loss: 0.7886 - val_accuracy: 0.6038 - val_loss: 0.6711\n",
            "Epoch 4/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6255 - loss: 0.6773 - val_accuracy: 0.5723 - val_loss: 0.7055\n",
            "Epoch 5/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6629 - loss: 0.6135 - val_accuracy: 0.5094 - val_loss: 0.7075\n",
            "Epoch 6/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6072 - loss: 0.6616 - val_accuracy: 0.5912 - val_loss: 0.6817\n",
            "Epoch 7/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6358 - loss: 0.6189 - val_accuracy: 0.5220 - val_loss: 0.7128\n",
            "Epoch 8/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5994 - loss: 0.6229 - val_accuracy: 0.5660 - val_loss: 0.7500\n",
            "Epoch 9/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6353 - loss: 0.6353 - val_accuracy: 0.5346 - val_loss: 0.7077\n",
            "Epoch 10/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6206 - loss: 0.5980 - val_accuracy: 0.5157 - val_loss: 0.9172\n",
            "Epoch 11/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6704 - loss: 0.6178 - val_accuracy: 0.6164 - val_loss: 0.6768\n",
            "Epoch 12/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6481 - loss: 0.5994 - val_accuracy: 0.6478 - val_loss: 0.6724\n",
            "Epoch 13/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6331 - loss: 0.6016 - val_accuracy: 0.5912 - val_loss: 0.6977\n",
            "Epoch 14/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6498 - loss: 0.5933 - val_accuracy: 0.5220 - val_loss: 0.7861\n",
            "Epoch 15/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6029 - loss: 0.6451 - val_accuracy: 0.6038 - val_loss: 0.6679\n",
            "Epoch 16/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6851 - loss: 0.5613 - val_accuracy: 0.5786 - val_loss: 0.6976\n",
            "Epoch 17/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6623 - loss: 0.5799 - val_accuracy: 0.5597 - val_loss: 0.6833\n",
            "Epoch 18/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7028 - loss: 0.5693 - val_accuracy: 0.5220 - val_loss: 0.7278\n",
            "Epoch 19/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6801 - loss: 0.5833 - val_accuracy: 0.5346 - val_loss: 0.7003\n",
            "Epoch 20/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6072 - loss: 0.6077 - val_accuracy: 0.5472 - val_loss: 0.7163\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5331 - loss: 0.7491\n",
            "ğŸ”¹ Neural Network Accuracy: 0.5472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEEP ENCODER"
      ],
      "metadata": {
        "id": "PA6OELlN75Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Add\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# ğŸš€ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ğŸ“‚ Define paths\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "autoencoder_model_path = '/content/drive/MyDrive/skin_cancer/deep_autoencoder.keras'\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/deep_encoder.keras'\n",
        "\n",
        "# ğŸ§¹ Clear GPU memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# ğŸ“Œ Autoencoder Architecture (Improved with Residual Connections)\n",
        "input_shape = (160, 160, 3)  # Keeping image size 160x160\n",
        "\n",
        "# ğŸ”¹ Encoder\n",
        "input_img = Input(shape=input_shape)\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)  # Extra Conv Layer\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)  # Extra Conv Layer\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "residual = x  # Store for residual connection\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)  # Extra Conv Layer\n",
        "x = Add()([x, residual])  # ğŸ”„ Residual Connection\n",
        "x = MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "bottleneck = Conv2D(256, (3,3), activation='relu', padding='same', name=\"encoder_output\")(x)\n",
        "\n",
        "# ğŸ”¹ Decoder\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(bottleneck)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "output_img = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# ğŸ”¹ Build Autoencoder Model\n",
        "autoencoder = Model(input_img, output_img)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# ğŸ”¹ Extract Encoder Model\n",
        "encoder = Model(input_img, bottleneck)\n",
        "\n",
        "# ğŸ”„ Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),\n",
        "    batch_size=16,  # ğŸš€ Increased Batch Size\n",
        "    class_mode='input',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# ğŸ”¥ Train Autoencoder\n",
        "autoencoder.fit(\n",
        "    data_generator,\n",
        "    epochs=20,  # ğŸš€ Increased epochs\n",
        "    steps_per_epoch=250  # ğŸš€ More steps per epoch\n",
        ")\n",
        "\n",
        "# ğŸ’¾ Save Models\n",
        "autoencoder.save(autoencoder_model_path)\n",
        "encoder.save(encoder_model_path)\n",
        "print(f\"âœ… Autoencoder saved at: {autoencoder_model_path}\")\n",
        "print(f\"âœ… Encoder saved at: {encoder_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2H-ObnY77Vm",
        "outputId": "a230839b-e786-4b4b-95c1-f7f314b092dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 793 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - loss: 0.0188 \n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - loss: 0.0079\n",
            "Epoch 3/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0056 \n",
            "Epoch 4/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0042 \n",
            "Epoch 5/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0041 \n",
            "Epoch 6/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0036 \n",
            "Epoch 7/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0038 \n",
            "Epoch 8/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0035 \n",
            "Epoch 9/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0033 \n",
            "Epoch 10/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0030 \n",
            "Epoch 11/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0032 \n",
            "Epoch 12/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0027 \n",
            "Epoch 13/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0024 \n",
            "Epoch 14/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0028 \n",
            "Epoch 15/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0025 \n",
            "Epoch 16/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0027 \n",
            "Epoch 17/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0023 \n",
            "Epoch 18/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0034 \n",
            "Epoch 19/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0030 \n",
            "Epoch 20/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0027 \n",
            "âœ… Autoencoder saved at: /content/drive/MyDrive/skin_cancer/deep_autoencoder.keras\n",
            "âœ… Encoder saved at: /content/drive/MyDrive/skin_cancer/deep_encoder.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define paths\n",
        "encoder_model_path = '/content/drive/MyDrive/skin_cancer/deep_encoder.keras'\n",
        "combined_folder = '/content/drive/MyDrive/skin_cancer_images/combined'\n",
        "csv_output_path = '/content/drive/MyDrive/skin_cancer/deep_extracted_features.csv'\n",
        "\n",
        "# Load trained encoder\n",
        "encoder = tf.keras.models.load_model(encoder_model_path)\n",
        "\n",
        "# Create ImageDataGenerator for loading images\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data_generator = datagen.flow_from_directory(\n",
        "    combined_folder,\n",
        "    target_size=(160, 160),\n",
        "    batch_size=1,  # Process one image at a time\n",
        "    class_mode='binary',  # Ensure labels are available\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Extract labels correctly\n",
        "labels = data_generator.classes  # Get numeric labels\n",
        "\n",
        "# Extract features\n",
        "features = encoder.predict(data_generator)\n",
        "\n",
        "# ğŸ”¥ Flatten features from (batch, 20, 20, 256) â†’ (batch, 102400)\n",
        "flattened_features = features.reshape(features.shape[0], -1)\n",
        "\n",
        "# Convert features to DataFrame\n",
        "df_features = pd.DataFrame(flattened_features)\n",
        "df_features['label'] = labels  # Add labels column (0 = benign, 1 = malignant)\n",
        "\n",
        "# Save to CSV\n",
        "df_features.to_csv(csv_output_path, index=False)\n",
        "print(f\"âœ… Features saved at: {csv_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxBj2mCv94vV",
        "outputId": "833adeca-f621-40c9-86de-7d37f68251a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 793 images belonging to 2 classes.\n",
            "\u001b[1m793/793\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step\n",
            "âœ… Features saved at: /content/drive/MyDrive/skin_cancer/deep_extracted_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load extracted features\n",
        "csv_path = '/content/drive/MyDrive/skin_cancer/deep_extracted_features.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=['label']).values  # Features\n",
        "y = df['label'].values  # Labels\n",
        "\n",
        "# Encode labels (convert class names to numbers)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Converts 'Benign' -> 0, 'Malignant' -> 1\n",
        "\n",
        "# Split dataset into training and testing (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train & Evaluate SVM\n",
        "svm_model = SVC(kernel='rbf', C=1.0)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "print(f\"ğŸ”¹ SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "\n",
        "# Train & Evaluate Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "print(f\"ğŸ”¹ Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# Train Neural Network (Simple Feedforward)\n",
        "nn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the neural network\n",
        "nn_model.fit(X_train, y_train, epochs=20, batch_size=8, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate Neural Network\n",
        "_, nn_accuracy = nn_model.evaluate(X_test, y_test)\n",
        "print(f\"ğŸ”¹ Neural Network Accuracy: {nn_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J_EHaml-I6y",
        "outputId": "a315908c-eb59-4cab-e97f-7834c50d0950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ SVM Accuracy: 0.6415\n",
            "ğŸ”¹ Random Forest Accuracy: 0.7107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.5919 - loss: 34.4127 - val_accuracy: 0.5849 - val_loss: 16.0694\n",
            "Epoch 2/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6242 - loss: 12.4858 - val_accuracy: 0.5723 - val_loss: 6.3749\n",
            "Epoch 3/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6587 - loss: 8.6109 - val_accuracy: 0.5472 - val_loss: 19.2726\n",
            "Epoch 4/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6122 - loss: 9.4667 - val_accuracy: 0.6226 - val_loss: 5.7936\n",
            "Epoch 5/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6815 - loss: 3.7966 - val_accuracy: 0.5409 - val_loss: 3.1303\n",
            "Epoch 6/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6958 - loss: 2.3529 - val_accuracy: 0.5409 - val_loss: 8.0423\n",
            "Epoch 7/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6023 - loss: 4.5495 - val_accuracy: 0.5849 - val_loss: 4.9257\n",
            "Epoch 8/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6953 - loss: 3.5683 - val_accuracy: 0.5094 - val_loss: 3.0722\n",
            "Epoch 9/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6905 - loss: 1.5639 - val_accuracy: 0.6101 - val_loss: 2.0046\n",
            "Epoch 10/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6407 - loss: 2.0746 - val_accuracy: 0.5472 - val_loss: 2.5911\n",
            "Epoch 11/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6812 - loss: 1.7228 - val_accuracy: 0.6289 - val_loss: 1.2294\n",
            "Epoch 12/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7284 - loss: 0.6410 - val_accuracy: 0.6289 - val_loss: 1.0332\n",
            "Epoch 13/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7841 - loss: 0.5550 - val_accuracy: 0.6604 - val_loss: 1.0295\n",
            "Epoch 14/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.4882 - val_accuracy: 0.6226 - val_loss: 0.9470\n",
            "Epoch 15/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7546 - loss: 0.5106 - val_accuracy: 0.5723 - val_loss: 1.1135\n",
            "Epoch 16/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.5978 - val_accuracy: 0.6415 - val_loss: 1.0843\n",
            "Epoch 17/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7707 - loss: 0.6458 - val_accuracy: 0.6478 - val_loss: 0.9125\n",
            "Epoch 18/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8004 - loss: 0.5207 - val_accuracy: 0.6667 - val_loss: 0.9743\n",
            "Epoch 19/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8220 - loss: 0.4915 - val_accuracy: 0.6415 - val_loss: 1.0381\n",
            "Epoch 20/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7839 - loss: 0.4858 - val_accuracy: 0.6604 - val_loss: 0.9421\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.6693 - loss: 0.8107\n",
            "ğŸ”¹ Neural Network Accuracy: 0.6604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"SVM Accuracy: 0.9180\")\n",
        "print(f\"Random Forest Accuracy: 0.8427\")\n",
        "print(f\"Neural Network Accuracy: 0.9119\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LASIsxSzBLS",
        "outputId": "6010c9e4-bee1-4697-b431-d124e7f09d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9180\n",
            "Random Forest Accuracy: 0.8427\n",
            "Neural Network Accuracy: 0.9119\n"
          ]
        }
      ]
    }
  ]
}